{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOvUWcxAyS275PmkwEs4LSI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### GGML로 변환한 모델 (.bin)을 양자화\n","\n","이전에 CPU에서 작동할 수 있도록 학습한 모델을 GGML 모델로 포팅한것을 그대로 쓰기에는 RAM이 부담.<br>\n","따라서, 부담을 줄이기 위해 4bit 양자화를 거쳐 GGML Quantization을 완성함"],"metadata":{"id":"qJLiB-UrGlxt"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"PoX2kLkaDs65","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691625814492,"user_tz":-540,"elapsed":19905,"user":{"displayName":"머홍","userId":"05066165938330005891"}},"outputId":"3b6da29d-4d28-4ebe-c7a4-b0348a582e01"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","source":["### GGML 포팅을 위해 Git을 Clone한다."],"metadata":{"id":"StgKtq9MG9-k"}},{"cell_type":"code","source":["# get the repo and build it\n","!git clone https://github.com/ggerganov/ggml\n","%cd ggml\n","!mkdir build\n","%cd build\n","!cmake ..\n","!make -j"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ck6H34qEU_3","executionInfo":{"status":"ok","timestamp":1691626215786,"user_tz":-540,"elapsed":45526,"user":{"displayName":"머홍","userId":"05066165938330005891"}},"outputId":"3736be8e-696a-4618-c5b9-d8e2ba85fd37"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'ggml'...\n","remote: Enumerating objects: 3026, done.\u001b[K\n","remote: Counting objects: 100% (1025/1025), done.\u001b[K\n","remote: Compressing objects: 100% (163/163), done.\u001b[K\n","remote: Total 3026 (delta 909), reused 910 (delta 844), pack-reused 2001\u001b[K\n","Receiving objects: 100% (3026/3026), 4.76 MiB | 22.06 MiB/s, done.\n","Resolving deltas: 100% (1992/1992), done.\n","/content/ggml\n","/content/ggml/build\n","-- The C compiler identification is GNU 11.4.0\n","-- The CXX compiler identification is GNU 11.4.0\n","-- Detecting C compiler ABI info\n","-- Detecting C compiler ABI info - done\n","-- Check for working C compiler: /usr/bin/cc - skipped\n","-- Detecting C compile features\n","-- Detecting C compile features - done\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Check for working CXX compiler: /usr/bin/c++ - skipped\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","-- Found Git: /usr/bin/git (found version \"2.34.1\") \n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n","-- Found Threads: TRUE  \n","-- CMAKE_SYSTEM_PROCESSOR: x86_64\n","-- x86 detected\n","-- Linux detected\n","-- x86 detected\n","-- Linux detected\n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/ggml/build\n","[  1%] \u001b[32mBuilding C object src/CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n","[  2%] \u001b[32mBuilding CXX object examples/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n","[  4%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n","[  5%] \u001b[32m\u001b[1mLinking C static library libggml.a\u001b[0m\n","[  5%] Built target ggml\n","[  5%] Built target common\n","[  6%] \u001b[32mBuilding C object tests/CMakeFiles/test-vec0.dir/test-vec0.c.o\u001b[0m\n","[  8%] \u001b[32mBuilding C object tests/CMakeFiles/test-vec1.dir/test-vec1.c.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding C object tests/CMakeFiles/test-mul-mat0.dir/test-mul-mat0.c.o\u001b[0m\n","[ 16%] \u001b[32mBuilding C object tests/CMakeFiles/test-mul-mat2.dir/test-mul-mat2.c.o\u001b[0m\n","[ 17%] \u001b[32mBuilding C object tests/CMakeFiles/test0.dir/test0.c.o\u001b[0m\n","[ 18%] \u001b[32mBuilding C object tests/CMakeFiles/test1.dir/test1.c.o\u001b[0m\n","[ 20%] \u001b[32mBuilding C object tests/CMakeFiles/test2.dir/test2.c.o\u001b[0m\n","[ 21%] \u001b[32mBuilding C object tests/CMakeFiles/test3.dir/test3.c.o\u001b[0m\n","[ 22%] \u001b[32mBuilding C object tests/CMakeFiles/test-pool.dir/test-pool.c.o\u001b[0m\n","[ 24%] \u001b[32mBuilding C object tests/CMakeFiles/test-customop.dir/test-customop.c.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object examples/CMakeFiles/common-ggml.dir/common-ggml.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object examples/whisper/CMakeFiles/whisper-cpp.dir/whisper.cpp.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object examples/mnist/CMakeFiles/mnist.dir/main.cpp.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CXX object examples/mnist/CMakeFiles/mnist-cpu.dir/main-cpu.cpp.o\u001b[0m\n","[ 31%] \u001b[32m\u001b[1mLinking C executable ../bin/test0\u001b[0m\n","[ 32%] \u001b[32m\u001b[1mLinking C executable ../bin/test-vec0\u001b[0m\n","[ 33%] \u001b[32m\u001b[1mLinking C executable ../bin/test2\u001b[0m\n","[ 33%] Built target test-vec0\n","[ 33%] Built target test0\n","[ 36%] \u001b[32m\u001b[1mLinking C executable ../bin/test3\u001b[0m\n","[ 35%] Built target test2\n","[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n","[ 37%] \u001b[32m\u001b[1mLinking C executable ../bin/test-customop\u001b[0m\n","[ 39%] \u001b[32m\u001b[1mLinking C executable ../bin/test-pool\u001b[0m\n","[ 39%] Built target test-opt\n","[ 39%] Built target test-customop\n","[ 39%] Built target test3\n","[ 39%] Built target test-pool\n","[ 40%] \u001b[32m\u001b[1mLinking C executable ../bin/test1\u001b[0m\n","[ 40%] Built target test1\n","[ 41%] \u001b[32m\u001b[1mLinking C executable ../bin/test-mul-mat0\u001b[0m\n","[ 41%] Built target test-mul-mat0\n","[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/mnist-cpu\u001b[0m\n","[ 44%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n","[ 44%] Built target mnist-cpu\n","[ 44%] Built target test-quantize-fns\n","[ 45%] \u001b[32m\u001b[1mLinking C executable ../bin/test-vec1\u001b[0m\n","[ 45%] Built target test-vec1\n","[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/mnist\u001b[0m\n","[ 47%] Built target mnist\n","[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grad0\u001b[0m\n","[ 48%] Built target test-grad0\n","[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n","[ 50%] Built target test-quantize-perf\n","[ 51%] \u001b[32m\u001b[1mLinking C executable ../bin/test-mul-mat2\u001b[0m\n","[ 51%] Built target test-mul-mat2\n","[ 52%] \u001b[32m\u001b[1mLinking CXX static library libcommon-ggml.a\u001b[0m\n","[ 52%] Built target common-ggml\n","[ 54%] \u001b[32mBuilding CXX object examples/gpt-2/CMakeFiles/gpt-2.dir/main.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object examples/gpt-j/CMakeFiles/gpt-j.dir/main.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object examples/whisper/CMakeFiles/whisper-quantize.dir/quantize.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object examples/gpt-neox/CMakeFiles/gpt-neox-quantize.dir/quantize.cpp.o\u001b[0m\n","[ 59%] \u001b[32mBuilding CXX object examples/gpt-2/CMakeFiles/gpt-2-quantize.dir/quantize.cpp.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object examples/gpt-neox/CMakeFiles/gpt-neox.dir/main.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object examples/gpt-j/CMakeFiles/gpt-j-quantize.dir/quantize.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object examples/dolly-v2/CMakeFiles/dollyv2.dir/main.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object examples/replit/CMakeFiles/replit.dir/main.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object examples/dolly-v2/CMakeFiles/dollyv2-quantize.dir/quantize.cpp.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object examples/replit/CMakeFiles/replit-quantize.dir/quantize.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object examples/mpt/CMakeFiles/mpt.dir/main.cpp.o\u001b[0m\n","[ 71%] \u001b[32mBuilding CXX object examples/starcoder/CMakeFiles/starcoder.dir/main.cpp.o\u001b[0m\n","[ 70%] \u001b[32mBuilding CXX object examples/mpt/CMakeFiles/mpt-quantize.dir/quantize.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object examples/starcoder/CMakeFiles/starcoder-mmap.dir/starcoder-mmap.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object examples/starcoder/CMakeFiles/starcoder-quantize.dir/quantize.cpp.o\u001b[0m\n","[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/replit-quantize\u001b[0m\n","[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/mpt-quantize\u001b[0m\n","[ 77%] Built target replit-quantize\n","[ 77%] Built target mpt-quantize\n","[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gpt-j-quantize\u001b[0m\n","[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gpt-2-quantize\u001b[0m\n","[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/dollyv2-quantize\u001b[0m\n","[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gpt-neox-quantize\u001b[0m\n","[ 82%] Built target gpt-j-quantize\n","[ 82%] Built target gpt-neox-quantize\n","[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/whisper-quantize\u001b[0m\n","[ 83%] Built target dollyv2-quantize\n","[ 83%] Built target gpt-2-quantize\n","[ 83%] Built target whisper-quantize\n","[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/starcoder-quantize\u001b[0m\n","[ 85%] Built target starcoder-quantize\n","[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gpt-j\u001b[0m\n","[ 86%] Built target gpt-j\n","[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/replit\u001b[0m\n","[ 87%] Built target replit\n","[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gpt-neox\u001b[0m\n","[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/starcoder-mmap\u001b[0m\n","[ 90%] Built target gpt-neox\n","[ 90%] Built target starcoder-mmap\n","[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gpt-2\u001b[0m\n","[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/starcoder\u001b[0m\n","[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/dollyv2\u001b[0m\n","[ 94%] Built target gpt-2\n","[ 94%] Built target starcoder\n","[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/mpt\u001b[0m\n","[ 95%] Built target dollyv2\n","[ 95%] Built target mpt\n","[ 97%] \u001b[32m\u001b[1mLinking CXX static library libwhisper-cpp.a\u001b[0m\n","[ 97%] Built target whisper-cpp\n","[ 98%] \u001b[32mBuilding CXX object examples/whisper/CMakeFiles/whisper.dir/main.cpp.o\u001b[0m\n","[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/whisper\u001b[0m\n","[100%] Built target whisper\n"]}]},{"cell_type":"markdown","source":["### 파이썬 필수 패키지 받기"],"metadata":{"id":"AQVyECWEHEQA"}},{"cell_type":"code","source":["# install Python dependencies\n","!python3 -m pip install -r ../requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nkM8JjbvE41I","executionInfo":{"status":"ok","timestamp":1691626234166,"user_tz":-540,"elapsed":18383,"user":{"displayName":"머홍","userId":"05066165938330005891"}},"outputId":"aefb28bd-fac1-4a67-d9c6-e5927d587029"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate==0.19.0 (from -r ../requirements.txt (line 1))\n","  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy==1.24.3 (from -r ../requirements.txt (line 2))\n","  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece==0.1.98 (from -r ../requirements.txt (line 3))\n","  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r ../requirements.txt (line 4)) (2.0.1+cu118)\n","Requirement already satisfied: torchaudio==2.0.2 in /usr/local/lib/python3.10/dist-packages (from -r ../requirements.txt (line 5)) (2.0.2+cu118)\n","Requirement already satisfied: torchvision==0.15.2 in /usr/local/lib/python3.10/dist-packages (from -r ../requirements.txt (line 6)) (0.15.2+cu118)\n","Collecting transformers==4.29.2 (from -r ../requirements.txt (line 7))\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.19.0->-r ../requirements.txt (line 1)) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.19.0->-r ../requirements.txt (line 1)) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.19.0->-r ../requirements.txt (line 1)) (6.0.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r ../requirements.txt (line 4)) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r ../requirements.txt (line 4)) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r ../requirements.txt (line 4)) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r ../requirements.txt (line 4)) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r ../requirements.txt (line 4)) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r ../requirements.txt (line 4)) (2.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2->-r ../requirements.txt (line 6)) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2->-r ../requirements.txt (line 6)) (9.4.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.29.2->-r ../requirements.txt (line 7))\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2->-r ../requirements.txt (line 7)) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.29.2->-r ../requirements.txt (line 7))\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2->-r ../requirements.txt (line 7)) (4.65.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r ../requirements.txt (line 4)) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r ../requirements.txt (line 4)) (16.0.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2->-r ../requirements.txt (line 7)) (2023.6.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->-r ../requirements.txt (line 4)) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2->-r ../requirements.txt (line 6)) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2->-r ../requirements.txt (line 6)) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2->-r ../requirements.txt (line 6)) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2->-r ../requirements.txt (line 6)) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->-r ../requirements.txt (line 4)) (1.3.0)\n","Installing collected packages: tokenizers, sentencepiece, numpy, huggingface-hub, transformers, accelerate\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Successfully uninstalled numpy-1.23.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\n","tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-0.19.0 huggingface-hub-0.16.4 numpy-1.24.3 sentencepiece-0.1.98 tokenizers-0.13.3 transformers-4.29.2\n"]}]},{"cell_type":"markdown","source":["### GGML 모델을 4bit Quantization화"],"metadata":{"id":"jfrWog0CHHFi"}},{"cell_type":"code","source":["!./bin/gpt-neox-quantize  \"/content/drive/MyDrive/KB 공모전/GGML/ggml-model-f16.bin\" \"/content/drive/MyDrive/KB 공모전/GGML/ggml-model-q4_0.bin\" \"q4_0\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PlDU5tdnFMZa","executionInfo":{"status":"ok","timestamp":1691626709368,"user_tz":-540,"elapsed":475219,"user":{"displayName":"머홍","userId":"05066165938330005891"}},"outputId":"8dd3e02c-2bda-4945-be92-7614908eacfc"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["gpt_neox_model_quantize: loading model from '/content/drive/MyDrive/KB 공모전/GGML/ggml-model-f16.bin'\n","gpt_neox_model_quantize: n_vocab     = 30080\n","gpt_neox_model_quantize: n_ctx       = 2048\n","gpt_neox_model_quantize: n_embd      = 5120\n","gpt_neox_model_quantize: n_head      = 40\n","gpt_neox_model_quantize: n_layer     = 40\n","gpt_neox_model_quantize: par_res     = 1\n","gpt_neox_model_quantize: ftype (src) = 1\n","gpt_neox_model_quantize: qntvr (src) = 0\n","gpt_neox_model_quantize: ftype (dst) = 2002\n","gpt_neox_model_quantize: qntvr (dst) = 2\n","                                        gpt_neox.embed_in.weight - [ 5120, 30080,     1], type =    f16 size =   587.50 MB ->    82.62 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n","                        gpt_neox.layers.0.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                          gpt_neox.layers.0.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","               gpt_neox.layers.0.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                 gpt_neox.layers.0.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.0.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n","                gpt_neox.layers.0.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                        gpt_neox.layers.0.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n","                          gpt_neox.layers.0.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                      gpt_neox.layers.0.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.027 0.041 0.059 0.079 0.099 0.112 0.117 0.110 0.094 0.074 0.054 0.037 0.024 0.020 \n","                        gpt_neox.layers.0.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                      gpt_neox.layers.0.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n","                        gpt_neox.layers.0.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                        gpt_neox.layers.1.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                          gpt_neox.layers.1.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","               gpt_neox.layers.1.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                 gpt_neox.layers.1.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.1.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                gpt_neox.layers.1.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                        gpt_neox.layers.1.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                          gpt_neox.layers.1.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                      gpt_neox.layers.1.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n","                        gpt_neox.layers.1.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                      gpt_neox.layers.1.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.035 0.011 0.017 0.027 0.040 0.059 0.087 0.140 0.211 0.136 0.084 0.057 0.039 0.026 0.017 0.015 \n","                        gpt_neox.layers.1.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                        gpt_neox.layers.2.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                          gpt_neox.layers.2.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","               gpt_neox.layers.2.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                 gpt_neox.layers.2.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.2.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                gpt_neox.layers.2.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                        gpt_neox.layers.2.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                          gpt_neox.layers.2.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                      gpt_neox.layers.2.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n","                        gpt_neox.layers.2.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                      gpt_neox.layers.2.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n","                        gpt_neox.layers.2.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                        gpt_neox.layers.3.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                          gpt_neox.layers.3.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","               gpt_neox.layers.3.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                 gpt_neox.layers.3.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.3.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n","                gpt_neox.layers.3.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                        gpt_neox.layers.3.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n","                          gpt_neox.layers.3.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                      gpt_neox.layers.3.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n","                        gpt_neox.layers.3.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                      gpt_neox.layers.3.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n","                        gpt_neox.layers.3.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                        gpt_neox.layers.4.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                          gpt_neox.layers.4.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","               gpt_neox.layers.4.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                 gpt_neox.layers.4.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.4.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n","                gpt_neox.layers.4.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                        gpt_neox.layers.4.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n","                          gpt_neox.layers.4.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                      gpt_neox.layers.4.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n","                        gpt_neox.layers.4.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                      gpt_neox.layers.4.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n","                        gpt_neox.layers.4.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                        gpt_neox.layers.5.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                          gpt_neox.layers.5.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","               gpt_neox.layers.5.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                 gpt_neox.layers.5.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.5.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                gpt_neox.layers.5.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                        gpt_neox.layers.5.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n","                          gpt_neox.layers.5.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                      gpt_neox.layers.5.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n","                        gpt_neox.layers.5.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                      gpt_neox.layers.5.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n","                        gpt_neox.layers.5.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                        gpt_neox.layers.6.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                          gpt_neox.layers.6.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","               gpt_neox.layers.6.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                 gpt_neox.layers.6.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.6.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                gpt_neox.layers.6.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                        gpt_neox.layers.6.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n","                          gpt_neox.layers.6.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                      gpt_neox.layers.6.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n","                        gpt_neox.layers.6.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                      gpt_neox.layers.6.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.113 0.097 0.077 0.056 0.038 0.025 0.020 \n","                        gpt_neox.layers.6.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                        gpt_neox.layers.7.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                          gpt_neox.layers.7.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","               gpt_neox.layers.7.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                 gpt_neox.layers.7.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.7.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                gpt_neox.layers.7.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                        gpt_neox.layers.7.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n","                          gpt_neox.layers.7.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                      gpt_neox.layers.7.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n","                        gpt_neox.layers.7.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                      gpt_neox.layers.7.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.119 0.113 0.097 0.077 0.056 0.038 0.025 0.020 \n","                        gpt_neox.layers.7.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                        gpt_neox.layers.8.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                          gpt_neox.layers.8.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","               gpt_neox.layers.8.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                 gpt_neox.layers.8.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.8.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                gpt_neox.layers.8.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                        gpt_neox.layers.8.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n","                          gpt_neox.layers.8.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                      gpt_neox.layers.8.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n","                        gpt_neox.layers.8.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                      gpt_neox.layers.8.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.077 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.024 0.020 \n","                        gpt_neox.layers.8.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                        gpt_neox.layers.9.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                          gpt_neox.layers.9.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","               gpt_neox.layers.9.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                 gpt_neox.layers.9.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.9.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                gpt_neox.layers.9.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                        gpt_neox.layers.9.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n","                          gpt_neox.layers.9.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                      gpt_neox.layers.9.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n","                        gpt_neox.layers.9.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                      gpt_neox.layers.9.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n","                        gpt_neox.layers.9.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.10.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.10.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.10.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.10.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.10.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.10.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.10.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n","                         gpt_neox.layers.10.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.10.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.10.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.10.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n","                       gpt_neox.layers.10.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.11.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.11.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.11.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.11.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.11.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.11.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.11.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n","                         gpt_neox.layers.11.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.11.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.11.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.11.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.098 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n","                       gpt_neox.layers.11.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.12.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.12.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.12.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.12.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.12.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.12.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.12.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.12.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.12.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.12.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.12.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.098 0.114 0.120 0.114 0.098 0.076 0.055 0.037 0.024 0.020 \n","                       gpt_neox.layers.12.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.13.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.13.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.13.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.13.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.13.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.13.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.13.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n","                         gpt_neox.layers.13.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.13.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.13.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.13.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.114 0.120 0.114 0.098 0.076 0.055 0.037 0.024 0.020 \n","                       gpt_neox.layers.13.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.14.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.14.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.14.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.14.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.14.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.14.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.14.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n","                         gpt_neox.layers.14.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.14.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.14.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.14.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.119 0.113 0.097 0.077 0.055 0.038 0.024 0.020 \n","                       gpt_neox.layers.14.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.15.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.15.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.15.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.15.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.15.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.15.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.15.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n","                         gpt_neox.layers.15.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.15.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.15.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.15.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n","                       gpt_neox.layers.15.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.16.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.16.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.16.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.16.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.16.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.16.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.16.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.16.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.16.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.16.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.16.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.114 0.120 0.114 0.098 0.076 0.055 0.037 0.024 0.020 \n","                       gpt_neox.layers.16.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.17.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.17.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.17.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.17.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.17.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.17.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.17.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n","                         gpt_neox.layers.17.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.17.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.17.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.17.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n","                       gpt_neox.layers.17.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.18.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.18.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.18.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.18.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.18.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.18.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.18.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.18.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.18.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.18.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.18.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.098 0.114 0.120 0.114 0.098 0.076 0.055 0.038 0.024 0.020 \n","                       gpt_neox.layers.18.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.19.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.19.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.19.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.19.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.19.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.19.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.19.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.19.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.19.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.19.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.19.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.114 0.120 0.114 0.098 0.076 0.055 0.037 0.024 0.020 \n","                       gpt_neox.layers.19.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.20.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.20.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.20.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.20.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.20.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.20.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.20.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.20.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.20.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.20.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.20.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.121 0.115 0.098 0.076 0.055 0.037 0.024 0.019 \n","                       gpt_neox.layers.20.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.21.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.21.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.21.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.21.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.21.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.21.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.21.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.21.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.21.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.21.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.21.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.114 0.121 0.114 0.098 0.076 0.055 0.037 0.024 0.020 \n","                       gpt_neox.layers.21.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.22.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.22.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.22.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.22.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.22.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.22.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.22.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.22.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.22.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.22.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.22.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.114 0.120 0.114 0.098 0.076 0.055 0.037 0.024 0.020 \n","                       gpt_neox.layers.22.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.23.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.23.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.23.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.23.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.23.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.23.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.23.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n","                         gpt_neox.layers.23.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.23.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.23.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.23.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.114 0.120 0.114 0.098 0.076 0.055 0.037 0.024 0.020 \n","                       gpt_neox.layers.23.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.24.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.24.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.24.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.24.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.24.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.24.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.24.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.24.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.24.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.24.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.24.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.098 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n","                       gpt_neox.layers.24.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.25.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.25.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.25.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.25.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.25.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.25.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.25.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n","                         gpt_neox.layers.25.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.25.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.25.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.25.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.024 0.020 \n","                       gpt_neox.layers.25.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.26.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.26.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.26.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.26.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.26.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.26.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.26.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.26.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.26.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.26.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.26.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.077 0.097 0.113 0.119 0.113 0.097 0.077 0.056 0.038 0.024 0.020 \n","                       gpt_neox.layers.26.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.27.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.27.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.27.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.27.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.27.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.27.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.27.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.27.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.27.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.27.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.27.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n","                       gpt_neox.layers.27.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.28.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.28.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.28.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.28.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.28.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.28.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.28.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n","                         gpt_neox.layers.28.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.28.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.28.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.28.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n","                       gpt_neox.layers.28.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.29.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.29.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.29.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.29.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.29.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.29.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.29.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.29.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.29.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.29.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.29.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n","                       gpt_neox.layers.29.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.30.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.30.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.30.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.30.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.30.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.30.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.30.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n","                         gpt_neox.layers.30.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.30.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.30.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.30.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n","                       gpt_neox.layers.30.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.31.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.31.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.31.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.31.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.31.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.31.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.31.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.31.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.31.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.31.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.31.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.31.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.32.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.32.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.32.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.32.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.32.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.32.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.32.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.32.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.32.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.32.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.32.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.32.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.33.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.33.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.33.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.33.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.33.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.33.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.33.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.33.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.33.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.33.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.33.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n","                       gpt_neox.layers.33.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.34.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.34.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.34.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.34.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.34.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.34.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.34.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.34.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.34.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.34.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.34.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n","                       gpt_neox.layers.34.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.35.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.35.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.35.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.35.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.35.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.35.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.35.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.35.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.35.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.35.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.35.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n","                       gpt_neox.layers.35.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.36.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.36.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.36.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.36.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.36.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.36.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.36.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.36.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.36.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.36.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.36.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n","                       gpt_neox.layers.36.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.37.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.37.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.37.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.37.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.37.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.37.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.37.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.37.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.37.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.37.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.37.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.076 0.098 0.116 0.124 0.116 0.098 0.076 0.054 0.036 0.023 0.019 \n","                       gpt_neox.layers.37.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.38.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.38.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.38.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.38.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.38.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","               gpt_neox.layers.38.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.38.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.38.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.38.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.38.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.38.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.014 0.022 0.035 0.053 0.075 0.099 0.119 0.127 0.119 0.099 0.075 0.053 0.035 0.022 0.018 \n","                       gpt_neox.layers.38.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                       gpt_neox.layers.39.input_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                         gpt_neox.layers.39.input_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","              gpt_neox.layers.39.post_attention_layernorm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                gpt_neox.layers.39.post_attention_layernorm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","             gpt_neox.layers.39.attention.query_key_value.weight - [ 5120, 15360,     1], type =    f16 size =   300.00 MB ->    42.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n","               gpt_neox.layers.39.attention.query_key_value.bias - [15360,     1,     1], type =    f32 size =    0.059 MB\n","                       gpt_neox.layers.39.attention.dense.weight - [ 5120,  5120,     1], type =    f16 size =   100.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                         gpt_neox.layers.39.attention.dense.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                     gpt_neox.layers.39.mlp.dense_h_to_4h.weight - [ 5120, 20480,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n","                       gpt_neox.layers.39.mlp.dense_h_to_4h.bias - [20480,     1,     1], type =    f32 size =    0.078 MB\n","                     gpt_neox.layers.39.mlp.dense_4h_to_h.weight - [20480,  5120,     1], type =    f16 size =   400.00 MB ->    56.25 MB | hist: 0.036 0.015 0.023 0.037 0.054 0.075 0.098 0.116 0.123 0.116 0.098 0.075 0.054 0.037 0.023 0.019 \n","                       gpt_neox.layers.39.mlp.dense_4h_to_h.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                                gpt_neox.final_layer_norm.weight - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                                  gpt_neox.final_layer_norm.bias - [ 5120,     1,     1], type =    f32 size =    0.020 MB\n","                                                embed_out.weight - [ 5120, 30080,     1], type =    f16 size =   587.50 MB ->    82.62 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.114 0.121 0.114 0.096 0.075 0.054 0.037 0.024 0.020 \n","ggml_common_quantize_0: model size  = 49185.20 MB\n","ggml_common_quantize_0: quant size  =  6925.43 MB | ftype = 2 (q4_0)\n","ggml_common_quantize_0: hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n","\n","main: quantize time = 475186.62 ms\n","main:    total time = 475186.62 ms\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"WAzqnHP2F70o"},"execution_count":null,"outputs":[]}]}